#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
Let there be some task over some state space 
\begin_inset Formula $S$
\end_inset

 that must be completed by two robotic agents, we call them expert agent
 
\begin_inset Formula $A_{e}$
\end_inset

 and student agent 
\begin_inset Formula $A_{s}$
\end_inset

.
\end_layout

\begin_layout Standard
The state space is divided into discrete states 
\begin_inset Formula $s\in S$
\end_inset

 and each agent has the same action space consisting of actions 
\begin_inset Formula $a\in A$
\end_inset


\end_layout

\begin_layout Standard
The reward function of this task is 
\begin_inset Formula $R(s)\space\forall s\in S$
\end_inset


\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The expert agent 
\begin_inset Formula $A_{e}$
\end_inset

 has performed some pre-training on this task, and knows the true reward
 function, and has some policy 
\begin_inset Formula $\pi(a|s)=\frac{exp(Q(s,a)/\tau)}{\sum exp(Q(s,b)/\tau)}\forall b\in A$
\end_inset


\end_layout

\begin_layout Standard
The task of the expert agent is not to perform the task optimally, for,
 given values of 
\begin_inset Formula $\tau$
\end_inset

 will make the agent perform the task 
\begin_inset Quotes eld
\end_inset

greedily
\begin_inset Quotes erd
\end_inset

 and therefore only choose the optimal actions.
\end_layout

\begin_layout Standard
The task of the expert agent is to demonstrate the true reward.
\end_layout

\begin_layout Standard
The task of the student agent is traditionally to replicate, but in actuality
 we think that is incorrect and the idea is to learn the reward.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
Deep Inverse Q Learning
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
For a given optimal action-value function Q* the corresponding distribution
 is:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\pi^{\epsilon}(a|s)=\frac{exp(Q^{*}(s,a))}{\sum exp(Q^{*}(s,A))}\forall a\in A
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Rearranging gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\sum exp(Q^{*}(s,A))=\frac{exp(Q^{*}(s,a))}{\pi^{\epsilon}(a|s)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
and 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
exp(Q^{*}(s,a))=\pi^{\epsilon}(a|s)\sum exp(Q^{*}(s,A))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Denoting 
\begin_inset Formula $b\in A\smallsetminus a$
\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
exp(Q^{*}(s,a))=\pi^{\epsilon}(a|s)\sum exp(Q^{*}(s,A))=\frac{\pi^{\epsilon}(a|s)}{\pi^{\epsilon}(b|s)}exp(Q^{*}(s,b))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Taking log gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Q^{*}(s,a)=Q^{*}(s,b)+\log(\pi^{\epsilon}(a|s)-\log(\pi^{\epsilon}(b|s))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We can then relate the optimal value of action a with the optimal values
 of all other actions in the same state by including the respective log
 probs
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
(n-1)Q^{*}(s,a)=(n-1)\log(\pi^{\epsilon}(a|s))+\sum Q^{*}(s,b)-\log(\pi^{\epsilon}(b|s))
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The optimal action-values for state s and action a are composed of immediate
 reward r(s,a) and the optimal action-value for the next state as given
 by the transition model
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Q^{*}(s,a)=R(s,a)+\gamma\max_{a'}\mathbb{E}_{s'\sim p(s'|s,a)}[Q^{*}(s',a')]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Replacing the Q values in the equation before above we can define the difference
 between the log-prob and discounted value of next state as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\eta_{s}^{a}\coloneqq\log(\pi^{\epsilon}(a|s))-\gamma\max_{a'}\mathbb{E}_{s'\sim p(s'|s,a)}[Q^{*}(s',a')]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
And thus solve for the immediate reward
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
R(s,a)=\eta_{s}^{a}+\frac{1}{n-1}\sum R(s,b)-\eta_{s}^{b}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
What's wrong with this image? Firstly, the notion of 
\begin_inset Formula $Q^{*}(s,a)$
\end_inset

 being optimal, the teaching policy is inherently suboptimal.
\end_layout

\begin_layout Standard
Secondly, we know the reward is actually 
\begin_inset Formula $R(s)$
\end_inset

 and not 
\begin_inset Formula $R(s,a)$
\end_inset

 in the paper, even over the environments tested.
 So whilst this paper presents a good imitation learner, in that its policy
 over the state space under the true reward is replicating the policy well...
 it isn't actually 
\begin_inset Quotes eld
\end_inset

understanding
\begin_inset Quotes erd
\end_inset

 and this notion is important for differently embodied agents to communicate
 the goal, as opposed to duplication.
\begin_inset Newline newline
\end_inset


\end_layout

\begin_layout Standard
The student and the teacher can thus only interact through the policy.
\end_layout

\begin_layout Standard
The teacher demonstrates, the student observes, the student demonstrates,
 the teacher observes and corrects.
\end_layout

\end_body
\end_document
