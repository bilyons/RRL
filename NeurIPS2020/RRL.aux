\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chentanez2005intrinsically}
\citation{pathak2017curiosity}
\citation{bialek1999predictive,der2012playful,klyubin2005empowerment}
\citation{blaes2019control}
\citation{ng2000algorithms}
\citation{leike2017ai,amodei2016concrete}
\citation{ziebart2008maximum}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{friston2006free}
\citation{still2012information}
\citation{bialek1999predictive}
\citation{tschantz2020reinforcement}
\citation{smith2018evaluation}
\citation{ng2000algorithms}
\citation{ng2000algorithms}
\citation{ziebart2008maximum}
\citation{pfau2016connecting}
\citation{klyubin2005empowerment}
\citation{salge2014empowerment}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\newlabel{background}{{2}{2}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{2}{subsection.2.1}\protected@file@percent }
\newlabel{invRL}{{2.2}{2}{Inverse RL}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Inverse RL}{2}{subsection.2.2}\protected@file@percent }
\newlabel{inRLeq}{{2}{2}{Inverse RL}{equation.2.2}{}}
\citation{kaelbling1998planning}
\citation{herrmann1995efficient}
\newlabel{emp}{{2.3}{3}{Discounted Empowerment}{subsection.2.3}{}}
\newlabel{gammares}{{2.3}{3}{Discounted Empowerment}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discounted Empowerment}{3}{subsection.2.3}\protected@file@percent }
\newlabel{eq:n_powerment}{{3}{3}{Discounted Empowerment}{equation.2.3}{}}
\newlabel{eq:gamma_powerment}{{4}{3}{Discounted Empowerment}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reflexive Reinforcement Learning}{3}{section.3}\protected@file@percent }
\newlabel{reflexive}{{3}{3}{Reflexive Reinforcement Learning}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic representation of RRL.\relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{rrlfig}{{1}{3}{Schematic representation of RRL.\relax }{figure.caption.2}{}}
\newlabel{Methods}{{4}{4}{Methods}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{4}{section.4}\protected@file@percent }
\newlabel{Actions_etc}{{4.1}{4}{Actions and Policy}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Actions and Policy}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Rewards}{4}{subsection.4.2}\protected@file@percent }
\newlabel{reward}{{5}{4}{Rewards}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Environments}{4}{subsection.4.3}\protected@file@percent }
\citation{klyubin2005empowerment}
\citation{klyubin2005empowerment}
\newlabel{Results}{{5}{5}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{5}{section.5}\protected@file@percent }
\newlabel{singlesect}{{5.1}{5}{Single Agent Learning}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Single Agent Learning}{5}{subsection.5.1}\protected@file@percent }
\newlabel{entres}{{5.1.1}{5}{Entropy}{subsubsection.5.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Entropy }{5}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Value function of the various arenas for the agent aiming to purely maximise entropy. \textbf  {(a)} is an empty arena. \textbf  {(b)} is a snaking obstacle. \textbf  {(c)} has a triangular obstacle with two corridors. \textbf  {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$. For the single agent experiments the exploration rate was $\varepsilon =0.75$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{entfigs}{{2}{5}{Value function of the various arenas for the agent aiming to purely maximise entropy. \textbf {(a)} is an empty arena. \textbf {(b)} is a snaking obstacle. \textbf {(c)} has a triangular obstacle with two corridors. \textbf {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$. For the single agent experiments the exploration rate was $\varepsilon =0.75$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{5}{subfigure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{5}{subfigure.2.4}\protected@file@percent }
\newlabel{gammares_exp}{{5.1.2}{5}{Discounted Empowerment}{subsubsection.5.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Discounted Empowerment}{5}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Further RRL Variants}{6}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Value function for an agent rewarded for obtaining sensory values of obstacles or walls in the environment, with the obstacles, episodes, episode length and parameters as in Fig.\nobreakspace  {}\ref  {entfigs}. Subfigures \textbf  {(a)-(d)} represent the prediction error reduction variants, and the subfigures \textbf  {(e)-(h)} represent the corner favouring variants.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{othervariants}{{3}{6}{Value function for an agent rewarded for obtaining sensory values of obstacles or walls in the environment, with the obstacles, episodes, episode length and parameters as in Fig.~\ref {entfigs}. Subfigures \textbf {(a)-(d)} represent the prediction error reduction variants, and the subfigures \textbf {(e)-(h)} represent the corner favouring variants.\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}{subfigure.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{6}{subfigure.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{6}{subfigure.3.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{6}{subfigure.3.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{6}{subfigure.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{6}{subfigure.3.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{6}{subfigure.3.8}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multi-Agent RRL}{6}{subsection.5.2}\protected@file@percent }
\newlabel{sharedspace}{{5.2}{6}{Multi-Agent RRL}{subsection.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Remarks}{6}{subsection.5.3}\protected@file@percent }
\citation{baird1995residual}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Value function for one of two agents rewarded for arriving in a nearby place to the other agent. Exploration rate is reduced: $\varepsilon = 0.50$ and other parameters are as in Fig.\nobreakspace  {}\ref  {entfigs}.. Both agents share a state-action value function.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{social}{{4}{7}{Value function for one of two agents rewarded for arriving in a nearby place to the other agent. Exploration rate is reduced: $\varepsilon = 0.50$ and other parameters are as in Fig.~\ref {entfigs}.. Both agents share a state-action value function.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{7}{subfigure.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{7}{subfigure.4.4}\protected@file@percent }
\newlabel{Discussion}{{6}{7}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{7}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Exploration vs.\nobreakspace  {}Exploitation}{7}{subsection.6.1}\protected@file@percent }
\citation{salge2014empowerment}
\citation{zhao2019learning}
\citation{klyubin2005all}
\citation{der2012playful}
\citation{klyubin2005empowerment}
\citation{smith2018evaluation}
\citation{balakrishnan2019incorporating}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Dynamical Effects of Reflexivity}{8}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Intrinsic Motivation}{8}{subsection.6.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\bibstyle{plainnat.bst}
\bibdata{bib_file}
\bibcite{amodei2016concrete}{{1}{2016}{{Amodei et~al.}}{{Amodei, Olah, Steinhardt, Christiano, Schulman, and Man{\'e}}}}
\bibcite{baird1995residual}{{2}{1995}{{Baird}}{{}}}
\bibcite{balakrishnan2019incorporating}{{3}{2019}{{Balakrishnan et~al.}}{{Balakrishnan, Bouneffouf, Mattei, and Rossi}}}
\bibcite{bialek1999predictive}{{4}{1999}{{Bialek and Tishby}}{{}}}
\bibcite{blaes2019control}{{5}{2019}{{Blaes et~al.}}{{Blaes, Pogan{\v {c}}i{\'c}, Zhu, and Martius}}}
\bibcite{chentanez2005intrinsically}{{6}{2005}{{Chentanez et~al.}}{{Chentanez, Barto, and Singh}}}
\bibcite{der2012playful}{{7}{2012}{{Der and Martius}}{{}}}
\bibcite{friston2006free}{{8}{2006}{{Friston et~al.}}{{Friston, Kilner, and Harrison}}}
\bibcite{herrmann1995efficient}{{9}{1995}{{Herrmann and Der}}{{}}}
\bibcite{kaelbling1998planning}{{10}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{klyubin2005all}{{11}{2005{}}{{Klyubin et~al.}}{{Klyubin, Polani, and Nehaniv}}}
\bibcite{klyubin2005empowerment}{{12}{2005{}}{{Klyubin et~al.}}{{Klyubin, Polani, and Nehaniv}}}
\bibcite{leike2017ai}{{13}{2017}{{Leike et~al.}}{{Leike, Martic, Krakovna, Ortega, Everitt, Lefrancq, Orseau, and Legg}}}
\bibcite{ng2000algorithms}{{14}{2000}{{Ng and Russell}}{{}}}
\bibcite{pathak2017curiosity}{{15}{2017}{{Pathak et~al.}}{{Pathak, Agrawal, Efros, and Darrell}}}
\bibcite{pfau2016connecting}{{16}{2016}{{Pfau and Vinyals}}{{}}}
\bibcite{salge2014empowerment}{{17}{2014}{{Salge et~al.}}{{Salge, Glackin, and Polani}}}
\bibcite{smith2018evaluation}{{18}{2019}{{Smith and Herrmann}}{{}}}
\bibcite{still2012information}{{19}{2012}{{Still and Precup}}{{}}}
\bibcite{tschantz2020reinforcement}{{20}{2020}{{Tschantz et~al.}}{{Tschantz, Millidge, Seth, and Buckley}}}
\bibcite{zhao2019learning}{{21}{2019}{{Zhao et~al.}}{{Zhao, Tiomkin, and Abbeel}}}
\bibcite{ziebart2008maximum}{{22}{2008}{{Ziebart et~al.}}{{Ziebart, Maas, Bagnell, and Dey}}}
