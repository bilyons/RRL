\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{ziebart2008maximum}
\citation{finn2016connection}
\citation{kakade2002natural,sutton2000policy}
\@writefile{toc}{\contentsline {section}{\numberline {A}Reflexive Reinforcement Learning (RRL)}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Inverse Reinforcement Learning (IRL)}{1}{subsection.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Numerical stability}{1}{subsection.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Noise level}{1}{subsection.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Maximum entropy IRL}{1}{subsection.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Markovianity}{1}{subsection.1.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Continuous States and Actions}{1}{subsection.1.6}\protected@file@percent }
\citation{Ravindran2001,mahajan2017symmetry}
\@writefile{toc}{\contentsline {section}{\numberline {B}Pseudocode}{2}{section.2}\protected@file@percent }
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Reflexive Reinforcement Learning\relax }}{2}{algorithm.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Additional Figures}{3}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Value function of the various arenas for the agent aiming to purely maximise $\gamma $-empowerment. \textbf  {(a)} is an empty arena. \textbf  {(b)} is a snaking obstacle. \textbf  {(c)} has a triangular obstacle with two corridors. \textbf  {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$. For the single agent experiments the exploration rate was $\varepsilon =0.75$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }}{3}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{empfigs}{{1}{3}{Value function of the various arenas for the agent aiming to purely maximise $\gamma $-empowerment. \textbf {(a)} is an empty arena. \textbf {(b)} is a snaking obstacle. \textbf {(c)} has a triangular obstacle with two corridors. \textbf {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$. For the single agent experiments the exploration rate was $\varepsilon =0.75$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}{subfigure.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{3}{subfigure.1.4}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Value function of the various arenas for the agents aiming to attempting to socialise. Agents are only rewarded for both choosing the action to greet the other within a Manhattan distance of 3. \textbf  {(a)} is an empty arena. \textbf  {(b)} is a snaking obstacle. \textbf  {(c)} has a triangular obstacle with two corridors. \textbf  {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$, with $\varepsilon =0.5$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }}{3}{figure.caption.2}\protected@file@percent }
\newlabel{twohi}{{2}{3}{Value function of the various arenas for the agents aiming to attempting to socialise. Agents are only rewarded for both choosing the action to greet the other within a Manhattan distance of 3. \textbf {(a)} is an empty arena. \textbf {(b)} is a snaking obstacle. \textbf {(c)} has a triangular obstacle with two corridors. \textbf {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room. Here and in the following, agents were trained for $10^7$ episodes each of a length of twice the size of the environment with discount factor $\gamma =0.9$ and learning rate $\alpha =0.1$, with $\varepsilon =0.5$. Yellow (blue) colour correspond to maximal (minimal) value at an arbitrary scale which is implied by the IRL scenario. All values are non-negative. Inaccessible regions of the environment are assumed to have zero value.\relax }{figure.caption.2}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{3}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{3}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{3}{subfigure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{3}{subfigure.2.4}\protected@file@percent }
\bibstyle{plainnat.bst}
\bibdata{bib_file}
\bibcite{finn2016connection}{{1}{2016}{{Finn et~al.}}{{Finn, Christiano, Abbeel, and Levine}}}
\bibcite{kakade2002natural}{{2}{2002}{{Kakade}}{{}}}
\bibcite{mahajan2017symmetry}{{3}{2017}{{Mahajan and Tulabandhula}}{{}}}
\bibcite{Ravindran2001}{{4}{2001}{{Ravindran and Barto}}{{}}}
\bibcite{sutton2000policy}{{5}{2000}{{Sutton et~al.}}{{Sutton, McAllester, Singh, and Mansour}}}
\bibcite{ziebart2008maximum}{{6}{2008}{{Ziebart et~al.}}{{Ziebart, Maas, Bagnell, and Dey}}}
