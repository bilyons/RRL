\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Background}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Reinforcement Learning}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Inverse RL}{section.2}% 4
\BOOKMARK [2][-]{subsection.2.3}{Discounted empowerment}{section.2}% 5
\BOOKMARK [1][-]{section.3}{Reflexive Reinforcement Learning}{}% 6
\BOOKMARK [1][-]{section.4}{Methods}{}% 7
\BOOKMARK [2][-]{subsection.4.1}{Actions and Policy}{section.4}% 8
\BOOKMARK [2][-]{subsection.4.2}{Rewards}{section.4}% 9
\BOOKMARK [2][-]{subsection.4.3}{Environments}{section.4}% 10
\BOOKMARK [1][-]{section.5}{Experiments}{}% 11
\BOOKMARK [2][-]{subsection.5.1}{Single agent learning}{section.5}% 12
\BOOKMARK [3][-]{subsubsection.5.1.1}{Entropy }{subsection.5.1}% 13
\BOOKMARK [3][-]{subsubsection.5.1.2}{Discounted empowerment}{subsection.5.1}% 14
\BOOKMARK [3][-]{subsubsection.5.1.3}{Flexibility of approach}{subsection.5.1}% 15
\BOOKMARK [2][-]{subsection.5.2}{Multi-agent RRL}{section.5}% 16
\BOOKMARK [2][-]{subsection.5.3}{Remarks}{section.5}% 17
\BOOKMARK [1][-]{section.6}{Discussion}{}% 18
\BOOKMARK [2][-]{subsection.6.1}{Exploration vs. exploitation}{section.6}% 19
\BOOKMARK [2][-]{subsection.6.2}{Intrinsic motivation}{section.6}% 20
\BOOKMARK [1][-]{section.7}{Conclusion}{}% 21
