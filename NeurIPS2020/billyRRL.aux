\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{chentanez2005intrinsically}
\citation{pathak2017curiosity}
\citation{bialek1999predictive,der2012playful,klyubin2005empowerment}
\citation{blaes2019control}
\citation{ng2000algorithms}
\citation{ziebart2008maximum}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{friston2006free}
\citation{still2012information}
\citation{bialek1999predictive}
\citation{tschantz2020reinforcement}
\citation{smith2018evaluation}
\citation{ng2000algorithms}
\citation{ng2000algorithms}
\citation{ziebart2008maximum}
\citation{pfau2016connecting}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Reinforcement Learning}{2}{subsection.2.1}\protected@file@percent }
\newlabel{invRL}{{2.2}{2}{Inverse RL}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Inverse RL}{2}{subsection.2.2}\protected@file@percent }
\newlabel{inRLeq}{{2}{2}{Inverse RL}{equation.2.2}{}}
\citation{klyubin2005empowerment}
\citation{salge2014empowerment}
\citation{kaelbling1998planning}
\citation{herrmann1995efficient}
\citation{balakrishnan2019incorporating}
\newlabel{emp}{{2.3}{3}{Discounted empowerment}{subsection.2.3}{}}
\newlabel{gammares}{{2.3}{3}{Discounted empowerment}{subsection.2.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discounted empowerment}{3}{subsection.2.3}\protected@file@percent }
\newlabel{eq:n_powerment}{{3}{3}{Discounted empowerment}{equation.2.3}{}}
\newlabel{eq:gamma_powerment}{{4}{3}{Discounted empowerment}{equation.2.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Reflexive Reinforcement Learning}{3}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Schematic representation of RRL.\relax }}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{rrlfig}{{1}{4}{Schematic representation of RRL.\relax }{figure.caption.2}{}}
\newlabel{Methods}{{4}{4}{Methods}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Methods}{4}{section.4}\protected@file@percent }
\newlabel{Actions_etc}{{4.1}{4}{Actions and Policy}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Actions and Policy}{4}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Rewards}{4}{subsection.4.2}\protected@file@percent }
\newlabel{reward}{{5}{4}{Rewards}{equation.4.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Environments}{5}{subsection.4.3}\protected@file@percent }
\newlabel{Results}{{5}{5}{Experiments}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{5}{section.5}\protected@file@percent }
\newlabel{singlesect}{{5.1}{5}{Single agent learning}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Single agent learning}{5}{subsection.5.1}\protected@file@percent }
\newlabel{entres}{{5.1.1}{5}{Entropy}{subsubsection.5.1.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Entropy }{5}{subsubsection.5.1.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces These colour maps represent the value of the various arenas the agent was placed in when aiming to purely maximise entropy. \textbf  {(a)} is an empty arena. \textbf  {(b)} is a snaking obstacle. \textbf  {(c)} has a triangular obstacle with two corridors. \textbf  {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{entfigs}{{2}{5}{These colour maps represent the value of the various arenas the agent was placed in when aiming to purely maximise entropy. \textbf {(a)} is an empty arena. \textbf {(b)} is a snaking obstacle. \textbf {(c)} has a triangular obstacle with two corridors. \textbf {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{5}{subfigure.2.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{5}{subfigure.2.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{5}{subfigure.2.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{5}{subfigure.2.4}\protected@file@percent }
\citation{klyubin2005empowerment}
\citation{klyubin2005empowerment}
\newlabel{gammares}{{5.1.2}{6}{Discounted empowerment}{subsubsection.5.1.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Discounted empowerment}{6}{subsubsection.5.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces These colour maps represent the value of the various arenas the agent was placed in when aiming to purely maximise $\gamma $-empowerment (\ref  {eq:gamma_powerment}), with the obstacles, episodes, episode length and parameters as in Fig.\nobreakspace  {}\ref  {entfigs}\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{entsimilarfigs}{{3}{6}{These colour maps represent the value of the various arenas the agent was placed in when aiming to purely maximise $\gamma $-empowerment (\ref {eq:gamma_powerment}), with the obstacles, episodes, episode length and parameters as in Fig.~\ref {entfigs}\relax }{figure.caption.4}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{6}{subfigure.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{6}{subfigure.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{6}{subfigure.3.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{6}{subfigure.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.3}Flexibility of approach}{6}{subsubsection.5.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Multi-agent RRL}{6}{subsection.5.2}\protected@file@percent }
\newlabel{sharedspace}{{5.2}{6}{Multi-agent RRL}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces These colour maps represent the value of the various arenas the agent was placed in when being rewarded for obtaining sensory values of obstacles or walls in the environment, with the obstacles, episodes, episode length and parameters as in Fig.\nobreakspace  {}\ref  {entfigs}. Subfigures \textbf  {(a)-(d)} represent the prediction error reduction variants, and the subfigures \textbf  {(e)-(h)} represent the corner favouring variants.\relax }}{7}{figure.caption.5}\protected@file@percent }
\newlabel{othervariants}{{4}{7}{These colour maps represent the value of the various arenas the agent was placed in when being rewarded for obtaining sensory values of obstacles or walls in the environment, with the obstacles, episodes, episode length and parameters as in Fig.~\ref {entfigs}. Subfigures \textbf {(a)-(d)} represent the prediction error reduction variants, and the subfigures \textbf {(e)-(h)} represent the corner favouring variants.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{subfigure.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{subfigure.4.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{7}{subfigure.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{7}{subfigure.4.4}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{7}{subfigure.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{7}{subfigure.4.6}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{7}{subfigure.4.7}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{7}{subfigure.4.8}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces These colour maps represent the value of the various arenas the agent were placed in when aiming to purely maximise entropy, where there are $10^7$ episodes, each being $42$ time steps long, $\varepsilon = 0.50, \gamma = 0.9, \alpha = 0.1$ . \textbf  {(a)} is an empty arena. \textbf  {(b)} is a snaking obstacle. \textbf  {(c)} has a triangular obstacle with two corridors. \textbf  {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room.. Here both agents are contributing to the same Q function and Value function at each update.\relax }}{7}{figure.caption.6}\protected@file@percent }
\newlabel{social}{{5}{7}{These colour maps represent the value of the various arenas the agent were placed in when aiming to purely maximise entropy, where there are $10^7$ episodes, each being $42$ time steps long, $\varepsilon = 0.50, \gamma = 0.9, \alpha = 0.1$ . \textbf {(a)} is an empty arena. \textbf {(b)} is a snaking obstacle. \textbf {(c)} has a triangular obstacle with two corridors. \textbf {(d)} is an arena consisting of two rooms, where the agent initialises in the smaller room.. Here both agents are contributing to the same Q function and Value function at each update.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{7}{subfigure.5.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{7}{subfigure.5.2}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{7}{subfigure.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{7}{subfigure.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Remarks}{7}{subsection.5.3}\protected@file@percent }
\citation{salge2014empowerment}
\citation{zhao2019learning}
\citation{klyubin2005all}
\newlabel{Discussion}{{6}{8}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{8}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Exploration vs.\nobreakspace  {}exploitation}{8}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Intrinsic motivation}{8}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}\protected@file@percent }
\citation{der2012playful}
\citation{klyubin2005empowerment}
\citation{smith2018evaluation}
\citation{baird1995residual}
\bibstyle{plainnat.bst}
\bibdata{bib_file}
\bibcite{baird1995residual}{{1}{1995}{{Baird}}{{}}}
\bibcite{balakrishnan2019incorporating}{{2}{2019}{{Balakrishnan et~al.}}{{Balakrishnan, Bouneffouf, Mattei, and Rossi}}}
\bibcite{bialek1999predictive}{{3}{1999}{{Bialek and Tishby}}{{}}}
\bibcite{blaes2019control}{{4}{2019}{{Blaes et~al.}}{{Blaes, Pogan{\v {c}}i{\'c}, Zhu, and Martius}}}
\bibcite{chentanez2005intrinsically}{{5}{2005}{{Chentanez et~al.}}{{Chentanez, Barto, and Singh}}}
\bibcite{der2012playful}{{6}{2012}{{Der and Martius}}{{}}}
\bibcite{friston2006free}{{7}{2006}{{Friston et~al.}}{{Friston, Kilner, and Harrison}}}
\bibcite{herrmann1995efficient}{{8}{1995}{{Herrmann and Der}}{{}}}
\bibcite{kaelbling1998planning}{{9}{1998}{{Kaelbling et~al.}}{{Kaelbling, Littman, and Cassandra}}}
\bibcite{klyubin2005all}{{10}{2005{}}{{Klyubin et~al.}}{{Klyubin, Polani, and Nehaniv}}}
\bibcite{klyubin2005empowerment}{{11}{2005{}}{{Klyubin et~al.}}{{Klyubin, Polani, and Nehaniv}}}
\bibcite{ng2000algorithms}{{12}{2000}{{Ng and Russell}}{{}}}
\bibcite{pathak2017curiosity}{{13}{2017}{{Pathak et~al.}}{{Pathak, Agrawal, Efros, and Darrell}}}
\bibcite{pfau2016connecting}{{14}{2016}{{Pfau and Vinyals}}{{}}}
\bibcite{salge2014empowerment}{{15}{2014}{{Salge et~al.}}{{Salge, Glackin, and Polani}}}
\bibcite{smith2018evaluation}{{16}{2019}{{Smith and Herrmann}}{{}}}
\bibcite{still2012information}{{17}{2012}{{Still and Precup}}{{}}}
\bibcite{tschantz2020reinforcement}{{18}{2020}{{Tschantz et~al.}}{{Tschantz, Millidge, Seth, and Buckley}}}
\bibcite{zhao2019learning}{{19}{2019}{{Zhao et~al.}}{{Zhao, Tiomkin, and Abbeel}}}
\bibcite{ziebart2008maximum}{{20}{2008}{{Ziebart et~al.}}{{Ziebart, Maas, Bagnell, and Dey}}}
